{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b43fd1c-4019-4e6e-99f9-c981b14249b1",
   "metadata": {
    "execution": {
     "parties": "[\"alice\"]",
     "shell.execute_reply.end": "2024-06-25T04:26:16.788988Z",
     "shell.execute_reply.started": "2024-06-25T04:26:16.781433Z",
     "to_execute": "2024-06-25T04:26:16.776Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     \"nodes\": {\n",
    "#         \"node:0\": \"127.0.0.1:9327\",\n",
    "#         \"node:1\": \"127.0.0.1:9328\",\n",
    "#     },\n",
    "#     \"devices\": {\n",
    "#         \"SPU\": {\n",
    "#             \"kind\": \"SPU\",\n",
    "#             \"config\": {\n",
    "#                 \"node_ids\": [\n",
    "#                     \"node:0\",\n",
    "#                     \"node:1\"\n",
    "#                 ],\n",
    "#                 \"spu_internal_addrs\": [\n",
    "#                     \"127.0.0.1:9327\",\n",
    "#                     \"127.0.0.1:9328\"\n",
    "#                 ],\n",
    "#                 \"runtime_config\": {\n",
    "#                     \"protocol\": \"SEMI2K\",\n",
    "#                     \"field\": \"FM64\",\n",
    "#                     \"enable_pphlo_profile\": true,\n",
    "#                     \"enable_pphlo_trace\": true,\n",
    "#                     \"enable_hal_profile\": true\n",
    "#                 }\n",
    "#             }\n",
    "#         },\n",
    "#         \"P1\": {\n",
    "#             \"kind\": \"PYU\",\n",
    "#             \"config\": {\n",
    "#                 \"node_id\": \"node:0\"\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "def twopc_def():\n",
    "    nodes_def =  {\n",
    "        \"node:0\": \"127.0.0.1:9327\",\n",
    "        \"node:1\": \"127.0.0.1:9328\",\n",
    "    }\n",
    "    devices_def = {\n",
    "        \"SPU\": {\n",
    "            \"kind\": \"SPU\",\n",
    "            \"config\": {\n",
    "                \"node_ids\": [\n",
    "                    \"node:0\",\n",
    "                    \"node:1\"\n",
    "                ],\n",
    "                \"spu_internal_addrs\": [\n",
    "                    \"127.0.0.1:9327\",\n",
    "                    \"127.0.0.1:9328\"\n",
    "                ],\n",
    "                \"runtime_config\": {\n",
    "                    \"protocol\": \"SEMI2K\",\n",
    "                    \"field\": \"FM64\",\n",
    "                    \"enable_pphlo_profile\": True,\n",
    "                    \"enable_pphlo_trace\": True,\n",
    "                    \"enable_hal_profile\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"P1\": {\n",
    "            \"kind\": \"PYU\",\n",
    "            \"config\": {\n",
    "                \"node_id\": \"node:0\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return nodes_def, devices_def\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c71f6-5a7d-4369-b0af-a93a5e8d3e95",
   "metadata": {
    "execution": {
     "parties": "[\"alice\"]",
     "shell.execute_reply.end": "2024-06-25T04:26:18.391025Z",
     "shell.execute_reply.started": "2024-06-25T04:26:16.790931Z",
     "to_execute": "2024-06-25T04:26:16.778Z"
    },
    "libroCellType": "markdown",
    "libroFormatter": "formatter-string"
   },
   "source": [
    "!pip install flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ba5224-84a1-4234-8e7f-cd9a8ef2785b",
   "metadata": {
    "execution": {
     "parties": "[\"alice\"]",
     "shell.execute_reply.end": "2024-06-25T04:26:20.112458Z",
     "shell.execute_reply.started": "2024-06-25T04:26:18.394741Z",
     "to_execute": "2024-06-25T04:26:16.778Z"
    },
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from torch.nn import Module, Conv2d, Linear, Softmax, ReLU,LayerNorm, Sequential\n",
    "import spu.utils.distributed as ppd\n",
    "import time\n",
    "\n",
    "class PatchEmbedding(Module):\n",
    "    def __init__ (\n",
    "        self,\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        in_channels,\n",
    "        embed_dim=768,\n",
    "        norm_layer=None,\n",
    "        flatten=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size= patch_size\n",
    "        self.grid_size=img_size // patch_size\n",
    "        self.num_patches =self.grid_size * self.grid_size\n",
    "        self.flatten = flatten\n",
    "\n",
    "        self.proj = Conv2d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "        )\n",
    "        self.norm_layer = norm_layer(embed_dim) if norm_layer else lambda x:x\n",
    "    def __call__ (self, inputs): \n",
    "        x = self.proj(inputs)\n",
    "        return inputs\n",
    "\n",
    "class GeLU(Module):\n",
    "    def __init__ (self,approximate = True):\n",
    "        super().__init__()\n",
    "        self.approximate = approximate\n",
    "    def __call__ (self,x):\n",
    "        if self.approximate:\n",
    "            sqrt_2_over_pi=np.sqrt(2 /np.pi).astype(x.dtype)\n",
    "            cdf =0.5 *(1.0 + jnp.tanh(sqrt_2_over_pi*(x + 0.044715 *(x** 3))))\n",
    "            return x*cdf\n",
    "\n",
    "\n",
    "class MultiHeadAttention(Module):\n",
    "    def init (\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        use_bias=True,\n",
    "        kdim=None,\n",
    "        vdim=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim= embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.kdim= kdim if kdim is not None else embed_dim\n",
    "        self.vdim= vdim if vdim is not None else embed_dim\n",
    "        self.use_bias = use_bias\n",
    "        self.head_dim=embed_dim // num_heads\n",
    "        # assert self.head dim *num heads == embed dim\n",
    "        self.q_proj= Linear(embed_dim, self.kdim, bias=use_bias)\n",
    "        self.k_proj= Linear(embed_dim, self.kdim, bias=use_bias)\n",
    "        self.v_proj= Linear(embed_dim,self.vdim, bias=use_bias)\n",
    "        self.out_proj= Linear(self.vdim, embed_dim, bias=use_bias)\n",
    "        self.softmax=Softmax(dim=-1)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def __call__ (self, x):\n",
    "        bs,seq_len,_=x.shape\n",
    "        # q，k，v projection\n",
    "        q= self.q_proj(x)\n",
    "        k= self.k_proj(x)\n",
    "        v= self.v_proj(x)\n",
    "        # softmax(qk^T)V\n",
    "        attn_logits =jnp.matmul(q,jnp.swapaxes(k,-2,-1))\n",
    "        attn_logits =attn_logits / jnp.sqrt(self.head_dim)\n",
    "        attn = self.softmax(attn_logits)\n",
    "        x= jnp.matmul(attn,v)\n",
    "        x=x.reshape(bs,seq_len,-1)\n",
    "        x= self.out_proj(x)\n",
    "        return x\n",
    "    # MLP layer\n",
    "\n",
    "# Transformer\n",
    "class Transformer(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=False,\n",
    "        act_layer=GeLU,\n",
    "        norm_layer=LayerNorm,\n",
    "        kdim=None,\n",
    "        vdim=None,\n",
    "    ):\n",
    "        self.norml = norm_layer(embed_dim)\n",
    "        self.attn =MultiHeadAttention(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            use_bias=qkv_bias,\n",
    "            kd_im=kdim,\n",
    "            vdim=vdim,\n",
    "        )\n",
    "        self.norm2= norm_layer(embed_dim)\n",
    "        self.mlp =Sequential(\n",
    "            Linear(embed_dim,int(embed_dim * mlp_ratio)),\n",
    "            GeLU(),\n",
    "            Linear(int(embed_dim * mlp_ratio),embed_dim)\n",
    "        )\n",
    "    def __call__(self,inputs):\n",
    "        inputs = inputs + self.attn(self.norm1(inputs))\n",
    "        inputs = inputs + self.mlp(self.norm2(inputs))\n",
    "        return inputs\n",
    "\n",
    "def main():\n",
    "    # init spu env\n",
    "    nodes_def, devices_def = twopc_def()\n",
    "    devices_def['SPU']['config']['runtime_config'][\"enable_action_trace\"]=True\n",
    "    ppd.init(nodes_def, devices_def)\n",
    "\n",
    "    # hyperparameters\n",
    "    embed_dim=65\n",
    "    num_tokens=65\n",
    "    # kdim, vdim=None, None\n",
    "    num_heads=4\n",
    "\n",
    "    # create an obj\n",
    "    func = MultiHeadAttention(\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads = num_heads,\n",
    "        use_bias=False,\n",
    "        kvim=embed_dim//num_heads,\n",
    "        vdim=embed_dim//num_heads\n",
    "    )\n",
    "\n",
    "    # assign device and func\n",
    "    spu_func=ppd.device(\"SPU\")(lambda: func)\n",
    "\n",
    "    # mpc computing\n",
    "    for i in range(10):\n",
    "        feat=np.random.randn(1,num_tokens, embed_dim)\n",
    "        plain_out=func(feat)\n",
    "\n",
    "        enc_feat=ppd.device(\"P1\")(lambda: feat)()\n",
    "        start=time.time()\n",
    "        enc_out=spu_func(enc_feat)\n",
    "        enc_out=ppd.get(enc_out)\n",
    "        \n",
    "        print(time.time()-start)\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
